{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "98f5e36a-da49-4ae2-8c74-b910a2f992fc",
      "metadata": {
        "id": "98f5e36a-da49-4ae2-8c74-b910a2f992fc"
      },
      "source": [
        "# Agent\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a router.\n",
        "\n",
        "* Our chat model will decide to make a tool call or not based upon the user input\n",
        "* We use a conditional edge to route to a node that will call our tool or simply end\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.44.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0ba0bd34b541c448cc_agent1.png)\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we can extend this into a generic agent architecture.\n",
        "\n",
        "In the above router, we invoked the model and, if it chose to call a tool, we returned a `ToolMessage` to the user.\n",
        "\n",
        "But, what if we simply pass that `ToolMessage` *back to the model*?\n",
        "\n",
        "We can let it either (1) call another tool or (2) respond directly.\n",
        "\n",
        "This is the intuition behind [ReAct](https://react-lm.github.io/), a general agent architecture.\n",
        "  \n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "This [general purpose architecture](https://blog.langchain.dev/planning-for-agents/) can be applied to many types of tools.\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63edff5a-724b-474d-9db8-37f0ae936c76",
      "metadata": {
        "id": "63edff5a-724b-474d-9db8-37f0ae936c76"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langgraph langgraph-prebuilt --trusted.host pypi.org --trusted.host files.pythonhosted.org\n",
        "%pip install --upgrade python-dotenv --trusted.host pypi.org --trusted.host files.pythonhosted.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356a6482",
      "metadata": {
        "id": "356a6482"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba35a12",
      "metadata": {
        "id": "dba35a12"
      },
      "source": [
        "Here, we'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing).\n",
        "\n",
        "We'll log to a project, `langchain-academy`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ebbb34",
      "metadata": {},
      "source": [
        "# Langsmith Setup\n",
        "\n",
        "1. Create a LangSmith account at [smith.langchain.com](https://smith.langchain.com/)\n",
        "2. Go to Tracing Projects\n",
        "3. Click on New Project and select the `tutorial` project\n",
        "4. Generate an API key for the project\n",
        "5. Copy the `LANGSMITH_API_KEY` and paste it into your `.env` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e6f1eb",
      "metadata": {
        "id": "60e6f1eb"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"tutorial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71795ff1-d6a7-448d-8b55-88bbd1ed3dbe",
      "metadata": {
        "id": "71795ff1-d6a7-448d-8b55-88bbd1ed3dbe"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math\n",
        "# the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/\n",
        "# play around with it and see how the model behaves with math equations!\n",
        "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2cec014-3023-405c-be79-de8fc7adb346",
      "metadata": {
        "id": "a2cec014-3023-405c-be79-de8fc7adb346"
      },
      "source": [
        "Let's create our LLM and prompt it with the overall desired agent behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d061813f-ebc0-432c-91ec-3b42b15c30b6",
      "metadata": {
        "id": "d061813f-ebc0-432c-91ec-3b42b15c30b6"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb43343-9a6f-42cb-86e6-4380f928633c",
      "metadata": {
        "id": "4eb43343-9a6f-42cb-86e6-4380f928633c"
      },
      "source": [
        "As before, we use `MessagesState` and define a `Tools` node with our list of tools.\n",
        "\n",
        "The `Assistant` node is just our model with bound tools.\n",
        "\n",
        "We create a graph with `Assistant` and `Tools` nodes.\n",
        "\n",
        "We add `tools_condition` edge, which routes to `End` or to `Tools` based on  whether the `Assistant` calls a tool.\n",
        "\n",
        "Now, we add one new step:\n",
        "\n",
        "We connect the `Tools` node *back* to the `Assistant`, forming a loop.\n",
        "\n",
        "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
        "* If it is a tool call, the flow is directed to the `tools` node.\n",
        "* The `tools` node connects back to `assistant`.\n",
        "* This loop continues as long as the model decides to call tools.\n",
        "* If the model response is not a tool call, the flow is directed to END, terminating the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef13cd4-05a6-4084-a620-2e7b91d9a72f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "aef13cd4-05a6-4084-a620-2e7b91d9a72f",
        "outputId": "898403d3-e63f-4990-e55d-726f5b4e8de3"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75602459-d8ca-47b4-9518-3f38343ebfe4",
      "metadata": {
        "id": "75602459-d8ca-47b4-9518-3f38343ebfe4"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b517142d-c40c-48bf-a5b8-c8409427aa79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b517142d-c40c-48bf-a5b8-c8409427aa79",
        "outputId": "178d9a69-95b1-4f87-c59b-344b1fc8b845"
      },
      "outputs": [],
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad869f22-9bfb-4cbe-9f30-8a307c5cdda2",
      "metadata": {
        "id": "ad869f22-9bfb-4cbe-9f30-8a307c5cdda2"
      },
      "source": [
        "## LangSmith\n",
        "\n",
        "We can look at traces in LangSmith."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
