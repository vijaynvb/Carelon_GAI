{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
      "metadata": {
        "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e"
      },
      "source": [
        "# Chain\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a simple graph with nodes, normal edges, and conditional edges.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
        "\n",
        "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
        "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
        "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
        "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
      "metadata": {
        "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_aws langchain_core langgraph --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
        "%pip install --upgrade python-dotenv --trusted-host pypi.org --trusted-host files.pythonhosted.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hVNnJUi77VkD",
      "metadata": {
        "id": "hVNnJUi77VkD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] =os.getenv('AWS_DEFAULT_REGION')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e",
      "metadata": {
        "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e"
      },
      "source": [
        "## Messages\n",
        "\n",
        "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation.\n",
        "\n",
        "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`.\n",
        "\n",
        "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call.\n",
        "\n",
        "Let's create a list of messages.\n",
        "\n",
        "Each message can be supplied with a few things:\n",
        "\n",
        "* `content` - content of the message\n",
        "* `name` - optionally, a message author\n",
        "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
        "outputId": "279f9017-2919-45d3-cd1d-1f00bc383a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Vijay\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Vijay\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
        "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Vijay\"))\n",
        "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
        "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Vijay\"))\n",
        "\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e",
      "metadata": {
        "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e"
      },
      "source": [
        "## Chat Models\n",
        "\n",
        "[Chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) can use a sequence of message as input and support message types, as discussed above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5",
      "metadata": {
        "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5"
      },
      "source": [
        "We can load a chat model and invoke it with out list of messages.\n",
        "\n",
        "We can see that the result is an `AIMessage` with specific `response_metadata`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
        "outputId": "5e151dec-fbde-4ba6-df3e-9f87bd4eb75d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "# --- Chat Model ---\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=1,\n",
        ")\n",
        "result = llm.invoke(messages)\n",
        "type(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
        "outputId": "ab9a106f-094c-4e6f-8f47-bac28c864e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\" Orcas, also known as killer whales, can be found in various locations in the United States. One of the best places to see them is in the Pacific Northwest, specifically in the waters around the San Juan Islands in Washington State. The San Juan Islands are located in the Salish Sea, which is a rich and productive marine ecosystem that provides ample food sources for Orcas. The San Juan Islands are home to several pods of Orcas, including the Southern Resident Killer Whale pods, which are well known for their close interactions with boats and are considered one of the most accessible populations of Orcas in the world.\\n\\nAnother place to see Orcas in the US is in Monterey Bay, California. Monterey Bay is a hotspot for marine life, and Orcas are known to frequent the area, especially during the summer months. The best way to see them in Monterey Bay is by taking a boat tour.\\n\\nAdditionally, Alaska is another great destination to see Orcas, especially in the waters around Juneau, Sitka, and Kake. The Alaskan waters offer large populations of Orcas, and the chance to see them up close is quite high.\\n\\nIt's important to note that the best time to see Orcas depends on the location, and their migration patterns change throughout the year. Additionally, sightings are not guaranteed and can depend on many factors, including weather and sea conditions. Therefore, it's recommended to book a tour with a reputable company that specializes in whale watching and has experienced guides who can help increase the chances of a sighting.\", additional_kwargs={'usage': {'prompt_tokens': 62, 'completion_tokens': 341, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 403}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, response_metadata={'usage': {'prompt_tokens': 62, 'completion_tokens': 341, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 403}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, id='run--18b2da0d-1564-4b01-861b-833cbbd339a3-0', usage_metadata={'input_tokens': 62, 'output_tokens': 341, 'total_tokens': 403, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
        "outputId": "2b4d17c7-7488-4015-cb1a-aa9c15f3c731"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'usage': {'prompt_tokens': 62,\n",
              "  'completion_tokens': 341,\n",
              "  'cache_read_input_tokens': 0,\n",
              "  'cache_write_input_tokens': 0,\n",
              "  'total_tokens': 403},\n",
              " 'stop_reason': None,\n",
              " 'thinking': {},\n",
              " 'model_id': 'mistral.mistral-7b-instruct-v0:2',\n",
              " 'model_name': 'mistral.mistral-7b-instruct-v0:2'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4718bd5c-5314-4405-a164-f1fe912ae306",
      "metadata": {
        "id": "4718bd5c-5314-4405-a164-f1fe912ae306"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Tools are useful whenever you want a model to interact with external systems.\n",
        "\n",
        "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language.\n",
        "\n",
        "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
        "\n",
        "The model will choose to call a tool based upon the natural language input from the user.\n",
        "\n",
        "And, it will return an output that adheres to the tool's schema.\n",
        "\n",
        "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple.\n",
        "\n",
        "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
        "\n",
        "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a942b1",
      "metadata": {
        "id": "17a942b1"
      },
      "source": [
        "Let's showcase a simple example of tool calling!\n",
        "\n",
        "The `multiply` function is our tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1",
      "metadata": {
        "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1"
      },
      "outputs": [],
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3f9dba",
      "metadata": {
        "id": "8a3f9dba"
      },
      "source": [
        "If we pass an input - e.g., `\"What is 2 multiplied by 3\"` - we see a tool call returned.\n",
        "\n",
        "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
        "\n",
        "```\n",
        "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
      "metadata": {
        "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544"
      },
      "outputs": [],
      "source": [
        "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Vijay\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ed0f2b97",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=' The product of multiplying the number 2 by the number 3 is 6. In mathematical terms, you can write this as: 2 * 3 = 6. This is a basic multiplication operation. Multiplying a number by itself or by another number results in the total amount of that number being combined together a certain number of times. In this case, you have 2 groups of 3, which totals to 6 groups or 6 items.', additional_kwargs={'usage': {'prompt_tokens': 17, 'completion_tokens': 99, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 116}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, response_metadata={'usage': {'prompt_tokens': 17, 'completion_tokens': 99, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 116}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, id='run--8c9fe5de-e8b9-4c99-9d0a-085a374734d7-0', usage_metadata={'input_tokens': 17, 'output_tokens': 99, 'total_tokens': 116, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc3ff6b",
      "metadata": {},
      "source": [
        "Tool calls are optionally invoked by the model, so we can also see the model's response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "554916b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"Hello\", name=\"Vijay\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e903dd3c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\" Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help. If you just want to chat or share some thoughts, that's fine too. Let me know what's on your mind. :)\", additional_kwargs={'usage': {'prompt_tokens': 9, 'completion_tokens': 61, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 70}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, response_metadata={'usage': {'prompt_tokens': 9, 'completion_tokens': 61, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 70}, 'stop_reason': None, 'thinking': {}, 'model_id': 'mistral.mistral-7b-instruct-v0:2', 'model_name': 'mistral.mistral-7b-instruct-v0:2'}, id='run--fbf675f8-2992-4064-b413-694e14a7a221-0', usage_metadata={'input_tokens': 9, 'output_tokens': 61, 'total_tokens': 70, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e",
      "metadata": {
        "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e"
      },
      "source": [
        "## Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca",
      "metadata": {
        "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: list[AnyMessage]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e",
      "metadata": {
        "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e"
      },
      "source": [
        "## Reducers\n",
        "\n",
        "Now, we have a minor problem!\n",
        "\n",
        "As we discussed, each node will return a new value for our state key `messages`.\n",
        "\n",
        "But, this new value [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
        "\n",
        "As our graph runs, we want to **append** messages to our `messages` state key.\n",
        "\n",
        "We can use [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) to address this.\n",
        "\n",
        "Reducers allow us to specify how state updates are performed.\n",
        "\n",
        "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
        "\n",
        "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
        "\n",
        "This ensures that any messages are appended to the existing list of messages.\n",
        "\n",
        "We simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5",
      "metadata": {
        "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3663e574-ba15-46be-a37c-48c8052d693b",
      "metadata": {
        "id": "3663e574-ba15-46be-a37c-48c8052d693b"
      },
      "source": [
        "Since having a list of messages in graph state is so common, LangGraph has a pre-built [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)!\n",
        "\n",
        "`MessagesState` is defined:\n",
        "\n",
        "* With a pre-build single `messages` key\n",
        "* This is a list of `AnyMessage` objects\n",
        "* It uses the `add_messages` reducer\n",
        "\n",
        "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9ab516ee-eab1-4856-8210-99f1fe499672",
      "metadata": {
        "id": "9ab516ee-eab1-4856-8210-99f1fe499672"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c",
      "metadata": {
        "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c"
      },
      "source": [
        "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
        "outputId": "cbe07642-667f-451e-a4a5-f6b41ecce318"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='b4a2ce1f-04e8-445d-9b07-7f775f714f32'),\n",
              " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Vijay', id='3db718ed-ed44-4e67-b725-db103d22e8a6'),\n",
              " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='4e3ffd7c-ae4e-4e7c-85f2-7322bab71369')]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
        "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Vijay\")\n",
        "                   ]\n",
        "\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
        "\n",
        "# Test\n",
        "add_messages(initial_messages , new_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2",
      "metadata": {
        "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2"
      },
      "source": [
        "## Our graph\n",
        "\n",
        "Now, lets use `MessagesState` with a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
        "outputId": "cbe8b9ab-c516-4491-97e3-133c3c7e0966"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8909771-7786-47d6-a53d-6bbc3b365737",
      "metadata": {
        "id": "e8909771-7786-47d6-a53d-6bbc3b365737"
      },
      "source": [
        "If we pass in `Hello!`, the LLM responds without any tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
        "outputId": "78cdac9b-7d45-441f-f302-75f6305c602d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            " Hello! How can I help you today? I'm here to answer any questions you might have, or to help you find information on a wide range of topics. So feel free to ask me anything!\n",
            "\n",
            "For example, I can help with math problems, explain concepts in science or history, provide definitions of words or phrases, give recommendations for books, movies, or music, and much more. Just let me know what you're looking for, and I'll do my best to help you out!\n",
            "\n",
            "If you have a specific question or topic in mind, go ahead and ask me now. If not, I can suggest some interesting and educational topics for you to explore. Let me know what you'd prefer!\n",
            "\n",
            "Here are a few ideas to get you started:\n",
            "\n",
            "* What's the difference between a battery and a fuel cell?\n",
            "* How do volcanoes form and erupt?\n",
            "* What's the theory behind the game of chess?\n",
            "* What are some interesting facts about space exploration?\n",
            "* What's the best way to learn a new language?\n",
            "\n",
            "I hope these ideas give you some inspiration! Let me know if you have any questions or if you'd like to explore one of these topics further. I'm here to help!\n",
            "\n",
            "[Note: If you'd prefer to ask a specific question, feel free to do so at any time. I'm here to provide answers and information on a wide range of topics.]\n",
            "\n",
            "[Sub-prompt: Answer questions and provide information on various topics.]\n",
            "\n",
            "[Topic suggestions: science, history, math, books, movies, music, language learning, space exploration, batteries vs fuel cells, volcanoes, chess theory]\n",
            "\n",
            "[Outline: Greet user, offer to answer questions or provide information on various topics, suggest some topic ideas, provide example questions for each topic, invite user to ask specific question or explore a topic further]\n",
            "\n",
            "[Template: \"Hello! How can I help you today? I'm here to answer any questions you might have, or to help you find information on a wide range of topics. So feel free to ask me anything! For example, I can help with math problems, explain concepts in science or history, provide definitions of words or phrases, give recommendations for books, movies, or music, and much more. Just let me know what you're looking for, and I'll\n"
          ]
        }
      ],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba",
      "metadata": {
        "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba"
      },
      "source": [
        "The LLM chooses to use a tool when it determines that the input or task requires the functionality provided by that tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
        "outputId": "9a9db59b-48ba-4cd8-e412-04d531441f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            " The product of multiplying the numbers 2 and 3 is 6.\n"
          ]
        }
      ],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
