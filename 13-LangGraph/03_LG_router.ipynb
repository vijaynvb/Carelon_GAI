{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce6fff79-25b5-4884-8aaa-e3ebb7ddd549",
      "metadata": {
        "id": "ce6fff79-25b5-4884-8aaa-e3ebb7ddd549"
      },
      "source": [
        "# Router\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a graph that uses `messages` as state and a chat model with bound tools.\n",
        "\n",
        "We saw that the graph can:\n",
        "\n",
        "* Return a tool call\n",
        "* Return a natural language response\n",
        "\n",
        "## Goals\n",
        "\n",
        "We can think of this as a router, where the chat model routes between a direct response or a tool call based upon the user input.\n",
        "\n",
        "This is a simple example of an agent, where the LLM is directing the control flow either by calling a tool or just responding directly.\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.09 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac6543c3d4df239a4ed1_router1.png)\n",
        "\n",
        "Let's extend our graph to work with either output!\n",
        "\n",
        "For this, we can use two ideas:\n",
        "\n",
        "(1) Add a node that will call our tool.\n",
        "\n",
        "(2) Add a conditional edge that will look at the chat model output, and route to our tool calling node or simply end if no tool call is performed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb4fc6e-7c85-4fc8-a4a9-0c7a527c4e5b",
      "metadata": {
        "id": "ebb4fc6e-7c85-4fc8-a4a9-0c7a527c4e5b"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_aws langchain_core langgraph langgraph-prebuilt --trusted-host pypi.org --trusted-host files.pythonhosted.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885e92d9",
      "metadata": {
        "id": "885e92d9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] =os.getenv('AWS_DEFAULT_REGION')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ba4df4-3045-49b1-9299-ced1fce14d24",
      "metadata": {
        "id": "e3ba4df4-3045-49b1-9299-ced1fce14d24"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=1,\n",
        ")\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77555a2",
      "metadata": {
        "id": "c77555a2"
      },
      "source": [
        " We use the [built-in `ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#toolnode) and simply pass a list of our tools to initialize it.\n",
        "\n",
        " We use the [built-in `tools_condition`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#tools_condition) as our conditional edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6fde4e-cceb-4426-b770-97ee4b41e9da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "9a6fde4e-cceb-4426-b770-97ee4b41e9da",
        "outputId": "8adbcc64-843e-4b8c-9cf4-3cb5fb710752"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_node(\"tools\", ToolNode([multiply]))\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_conditional_edges(\n",
        "    \"tool_calling_llm\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b608c5-0c15-4fb7-aa24-80ce5774fb85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11b608c5-0c15-4fb7-aa24-80ce5774fb85",
        "outputId": "819f91e3-3495-4802-ec0a-c975adc664a6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "messages = [HumanMessage(content=\"Hello, what is 2 multiplied by 2?\")]\n",
        "messages = graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34708377-16b6-4474-9e23-71890c1fb36e",
      "metadata": {
        "id": "34708377-16b6-4474-9e23-71890c1fb36e"
      },
      "source": [
        "Now, we can see that the graph runs the tool!\n",
        "\n",
        "It responds with a `ToolMessage`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
