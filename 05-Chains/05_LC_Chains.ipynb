{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ZZDU_ajpYu9F",
      "metadata": {
        "id": "ZZDU_ajpYu9F"
      },
      "source": [
        "# **Chains**\n",
        "\n",
        "chains are a fundamental concept that allows you to execute complex tasks in a structured and efficient way.\n",
        "\n",
        "## **Why Chains?**\n",
        "\n",
        "Chains are invaluable due to their capacity to effortlessly blend diverse components, shaping a singular and coherent application. Through the creation of chains, multiple elements can seamlessly come together. Imagine this scenario: a chain is crafted to take in user input, polish it using a PromptTemplate, and subsequently pass on this refined response to a large language model (LLM). This streamlined process not only simplifies but also enriches the overall functionality of the system. In essence, chains serve as the linchpin, seamlessly connecting different parts of the application and enhancing its capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "548b1c6c",
      "metadata": {
        "id": "548b1c6c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "%pip install --upgrade langchain langchain_community langchain_aws --trusted.host pypi.org --trusted.host files.pythonhosted.org\n",
        "%pip install --upgrade python-dotenv --trusted.host pypi.org --trusted.host files.pythonhosted.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oE8SwJDrv8Mx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "oE8SwJDrv8Mx",
        "outputId": "aa602fc3-fee8-4d78-8034-457c6ca0127f"
      },
      "outputs": [],
      "source": [
        "%pip uninstall -y numpy pandas --trusted.host pypi.org --trusted.host files.pythonhosted.org\n",
        "%pip install --no-cache-dir numpy==1.26.4 pandas --trusted.host pypi.org --trusted.host files.pythonhosted.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40224a75",
      "metadata": {
        "id": "40224a75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] =os.getenv('AWS_DEFAULT_REGION')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3485d1af",
      "metadata": {
        "id": "3485d1af"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"../content/employee.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4559e482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "4559e482",
        "outputId": "e89ded26-abc4-4734-a583-47a441ceeafc"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PrxH7ELIZABK",
      "metadata": {
        "id": "PrxH7ELIZABK"
      },
      "source": [
        "# **LLM Chain** - **The simplest chain**\n",
        "\n",
        "The LLMChain is a foundational system that includes a PromptTemplate, an OpenAI model (such as a Large Language Model or a ChatModel), and optionally, an output parser. It operates by transforming input parameters using the PromptTemplate into a coherent prompt, which is then fed into the model. The resulting output is further refined and formatted into a usable form by the OutputParser, if provided. This structured approach ensures effective utilization of language models for various applications, enhancing their functionality and utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201646f3",
      "metadata": {
        "id": "201646f3"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437ec9e0",
      "metadata": {
        "id": "437ec9e0"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18955fef",
      "metadata": {
        "id": "18955fef"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the tools that need to be learned to earn the {designation}?,provide me one tool\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa1076c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fa1076c",
        "outputId": "cb27359f-d779-4450-a23f-99470b19e684"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "786b37fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "786b37fb",
        "outputId": "1f68bd7f-7b1a-4385-ec59-89398907ad06"
      },
      "outputs": [],
      "source": [
        "designation = \"Devops Engineer\"\n",
        "chain.run(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cFmIwlFHZn1h",
      "metadata": {
        "id": "cFmIwlFHZn1h"
      },
      "source": [
        "# **SimpleSequentialChain**\n",
        "\n",
        "Simple Sequential Chains allow for a single input to undergo a series of coherent transformations, resulting in a refined output. This sequential approach ensures systematic and efficient handling of data, making it ideal for scenarios where a linear flow of information processing is essential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e6d036",
      "metadata": {
        "id": "16e6d036"
      },
      "outputs": [],
      "source": [
        "# SimpleSequentialChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb809100",
      "metadata": {
        "id": "eb809100"
      },
      "outputs": [],
      "source": [
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the tools that need to be learned to earn the {designation}?,provide me one tool\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b61f827",
      "metadata": {
        "id": "1b61f827"
      },
      "outputs": [],
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    tool:{tool_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6be59d",
      "metadata": {
        "id": "ba6be59d"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c07daae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "9c07daae",
        "outputId": "9e08b9bd-04a0-4792-931b-d48971bcdb04"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain.run(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1d_gTGOZ3ZZ",
      "metadata": {
        "id": "i1d_gTGOZ3ZZ"
      },
      "source": [
        "# **SequentialChain**\n",
        "\n",
        "A sequential chain is a chain that combines various individual chains, where the output of one chain serves as the input for the next in a continuous sequence. It operates by running a series of chains consecutively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb38a41",
      "metadata": {
        "id": "dcb38a41"
      },
      "outputs": [],
      "source": [
        "# SequentialChain\n",
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db3f34",
      "metadata": {
        "id": "42db3f34"
      },
      "outputs": [],
      "source": [
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following designation\"\n",
        "    \"\\n\\n{Designation}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"Designation_description\"\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed023b4",
      "metadata": {
        "id": "eed023b4"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following description in 1 sentence:\"\n",
        "    \"\\n\\n{Designation_description}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad04be0",
      "metadata": {
        "id": "5ad04be0"
      },
      "outputs": [],
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the promgramming language that need to be learned for this designation in one line:\\n\\n{Designation}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"programming_language\"\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b0105f",
      "metadata": {
        "id": "13b0105f"
      },
      "outputs": [],
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short follow up response to the following \"\n",
        "    \"Write a short summary about the programming language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {programming_language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d175021",
      "metadata": {
        "id": "2d175021"
      },
      "outputs": [],
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Designation\"],\n",
        "    output_variables=[\"Designation_description\",\"programming_language\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2fb50d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2fb50d",
        "outputId": "bd5228c6-7dfc-4cf2-fdef-0ef27a2b00ea",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "designation = df.Designation[2]\n",
        "overall_chain(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a492e1",
      "metadata": {},
      "source": [
        "# **Conversation Chain**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78bca8e",
      "metadata": {},
      "source": [
        "ConversationChain with ConversationBufferMemory keeps in memory the previous LLMs answer. Combine all the previous question and LLM’s answer in a new prompt and pass it again to the LLM for the current answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4767f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85ecdbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose= True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be3238f",
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi, my name is Vijay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UDDmuisndnbJ",
      "metadata": {
        "id": "UDDmuisndnbJ"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Practice using chains with language models and structured output parsing to handle complex tasks efficiently.\n",
        "\n",
        "## **Scenario**\n",
        "\n",
        "You are tasked with building a system to assist users in understanding various technical concepts related to software engineering roles. Your goal is to use different chains to process user queries, transform them using prompt templates, and extract specific information using output parsers.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* Define a Prompt Template\n",
        "* Prepare Sample Queries\n",
        "* Implement LLM Chains\n",
        "* Output Parsing\n",
        "* Interactive Chain Execution\n",
        "* Evaluate"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
